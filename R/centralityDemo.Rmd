---
title: "Network centrality"
output:
  html_notebook:
    toc: yes
    toc_float: yes
    theme: united
  html_document:
    toc: yes
    df_print: paged
bibliography: workshop.bib
---

```{r setup, include = FALSE, warning=FALSE, message=FALSE}
# load source files
source('./setup.R')
source('./functions/networkFunctions.R')
source('./functions/miscFunctions.R')
```
The following notebook will demonstrate how directed weighted graphs can be constructed from word association data. The weights reflect the proportion of people that give a response to each cue word (we'll cover alternatives in the similarityDemo).

We'll use this network to illustrate different ways in which nodes can be *central*. Over [403 network centrality measures](https://www.centiserver.org/) are currently available and the list keeps growing [@Jalili2015]. We'll focus on the most commonly used ones and consider versions that are useful for directed weighted graphs.


# Load network
For this example, we will use the USF network [@Nelson2004] as some centrality measures take a long time to compute on larger graphs like SWOW (> 10,000 nodes). 

```{r loadData, echo=TRUE, include=TRUE}
load('../data/WAT/USF.rda') %>% as_tibble()
X = USF
DT::datatable(X %>% filter(cue=='island'))

```

Many studies remove idiosyncratic responses, that is, response generated by a single participant. For the example *island*, there are 18 responses given only once out of 34.
Given that most response distributions have a heavy tail, this large number of 
idiosyncratic responses is entirely expected. Furthermore, as you can judge yourself,
they are far from arbitrary. For this reason, it often makes sense to include them
and avoid any additional researcher degree's of freedom.



To build a graph, all we need is a table where each row defines an edge based on a source (cue) and (target), together with an optional weight (R1.Strength). Before we proceed, 
 we need to make sure the graph is simple (i.e. it does not contain any loops). 


```{r constructGraph, echo=TRUE, include=FALSE}
load('../data/WAT/USF.rda') %>% as_tibble()
X = USF

# Remove loops
X %<>% filter(cue != response)

# Note the funny %<>% operator.
# This is a  magrittr compound pipe operators 
# allowing you to shorten the folling
# X = X %>% filter(cue != response)

# Recalculate strength to sum to 1 for each node.
cue.freq = X %>% group_by(cue) %>% summarise(total = sum(R1))
X = left_join(X,cue.freq,by = 'cue') %>% mutate(R1.Strength =  R1/total)

# Import the graph
G = graph_from_data_frame(X %>% select(from = cue,to=response),directed = T)
G = set_edge_attr(G,'weight',value = X$R1.Strength)

```

The adjacency matrix is a different representation of a graph where each row and column corresponds to a node, and the cells represent the absence or presence of an edge. In this case, these cells are values between 0 and 1 suggesting a *weighted* adjacency matrix.
```{r adjacency}
G[c('island','sun','tropical','vacation','fern','office'),
  c('island','sun','tropical','vacation','fern','office')]
```


In some cases, participants did respond with the same word as the cue word, so these are removed.


## Graph connectedness
```{r graphConnectedness, echo=FALSE}

# Extract component
Comp = extractComponent(G,mode = 'strong')

# Reduce to the largest connected component
G  = Comp$subGraph


# Check which vertices are removed
Censored = X %>% group_by(response) %>% summarise(Freq = sum(R1)) %>% 
  filter(Freq > 1, response %in% Comp$removedVertices) %>% 
  arrange(-Freq)


# Since this removes nodes, it affects the normalized associative strength edge weights.
# Let's recompute using a sourced utility function:
G = normalizeEdgeWeights(G)


# Check: 
#sum(G.eat['apple',])

vlabels = igraph::V(G)$name

```

### Graph components
Isolated nodes that do not have either incoming or outgoing links or both. 
The list contains `r length(Comp$removedVertices)` nodes with *neither* incoming or outgoing edges.

A related idea is that of **graph components**. A component of a graph associated with a node $v_i$ is the maximal connected induced subgraph containing all nodes that are connected to $v_i$. In directed graphs, the *weakly connected component* does not consider whether an edge is in- or out-going, whereas the *strongly connected component* requires that all nodes in the component are connected through in- and out-going edges. Finally, the *largest connected component* is the induced subgraph which contains the maximum number of nodes.

The word association network is a directed graph and many directed centrality measures can only be calculated if all nodes are part of a strongly connected component. The original order of the graph (i.e. the number of nodes) is `r vcount(G)`.

Let's extract the largest strongly connected component.
In this case, there's one large component, and 52 components that correspond to the isolated vertices identified above. The number of nodes after extracting the largest connected components is `r vcount(G)`. 

### Coverage
Extracting the strongest component means each word has at least one in- and out-going link $k_{in} \geq 1, k_{out} \geq 1$. This implies that only a subset of response words is retained, namely those that are in the list of cues that occur at least once in the list of responses. It is important to reflect on what this censoring means when calculating different types of centrality estimates.

```{r calculateCoverage}
X %<>% mutate(isCue = ifelse(response %in% X$cue,1,0))

X.cover = X %>% filter(isCue ==1) %>% 
  group_by(cue) %>% 
  summarise(coverage = sum(R1.Strength))

DT::datatable(X.cover)
```


```{r plotCoverage, fig.show=T,fig.width=5}
fig.coverage = ggplot(X.cover, aes(x = coverage)) + 
  geom_histogram(color = 'black',bins = 30,alpha = 0.6) + 
  geom_vline(xintercept = median(X.cover$coverage), linetype="dashed", 
                color = "orangered2", size=0.7) +
  theme_bw()

print(fig.coverage)
```


For this dataset, the median coverage is `r `median(X.cover$coverage)`, which suggests that the amount of censoring is relatively small.

## Macro-level descriptives
The macro-level descriptives provide information about the number of nodes and edges, the density of the network and the diameter.

```{r globalDescriptives, message = FALSE}

S.global = list()
S.global$nEdges = ecount(G)
S.global$nNodes = vcount(G)
S.global$density = edge_density(G)
S.global$diameterPath = get_diameter(G)

#S.global$diameter = diameter(G, weights = NULL) # weighted diameter or length of the longest geodesic (i.e shortest path)

S.global$cc = transitivity(G,'global') # global clustering coefficient

#print.simple.list(S.global)
```

The network consists of `r S.global$nNodes` nodes and `r S.global$nEdges` edges.
The density of the network is `r S.global$density` and the overal transitivity, that
 is the probability that neighbors of nodes are connected (also referred to as the clustering coefficient is) `r S.global$cc`.
In this network the longest path has a length of `r length(S.global$diameterPath)`:  
`r paste(names(S.global$diameterPath),collapse = ' -> ')`.


## Local centrality measures
Local centrality measures considers how a node is connected with its direct neighbors. This includes measures such as the number of incoming edges (`in-degree` $k_{in}$), the number of outgoing edges (`out-degree` $k_{out}) and weighted versions of these measures (*strength*).

<center>
![Illustration of in and out-degree](../figures/in_out_degree.jpg)
</center>

```{r centralityTable, warning=FALSE}
ct_k = tibble(names  = vlabels,
                    degree_in = igraph::degree(G, mode = 'in'),
                    degree_out = igraph::degree(G, mode = 'out'),
                    strength_in = igraph::strength(G,mode = 'in'),
                    strength_out = igraph::strength(G, mode = 'out'))

DT::datatable(ct_k)  
```

Not surprisingly, the out-strength is the same for each node. This is because the summed associative strengths for each node are normalized to sum to one.



## Global centrality measures
In contrast to local centrality measures which only count the number and the strength of a node and its direct neighbors, *global* centrality measures consider the centrality of the neighbors as well to determine how central a node $v_i$ is. There are hundreds of ways of calculating centrality, resulting in highly related measures that capture slightly different aspects about what nodes are important.

<center>
![Illustration of various centrality measures](../figures/centrality.png)
</center>


## Neighborhood size
One idea is to consider the neighbors of neighbors in calculating degree and strength.

```{r NBCentralityTable, warning=FALSE}
ct_nb = tibble(names = vlabels,
                nb3_out = igraph::neighborhood.size(G,order = 3, mode = 'out'),
                nb3_in = igraph::neighborhood.size(G,order = 3, mode = 'in'),
                nb2_out = igraph::neighborhood.size(G,order = 2, mode = 'out'),
                nb2_in = igraph::neighborhood.size(G,order = 2, mode = 'in'))

DT::datatable(cbind(ct_nb$names,round(ct_nb %>% select(-names),5)))
```
Large out-neighborhoods tend to correspond to fairly abstract words.
Some words, like *trance* have few neighbors, even up to three steps.
Note that EAT and USF are based on single word associations procedures, which tends to
underestimate weaker links.

```{r echo=F, fig.show=T,fig.align='center'}
lonelyWord = 'scallop'
G.small_nb = igraph::make_ego_graph(G,order= 2,nodes =lonelyWord,mode ='in')[[1]]

# change geom_edge_link to geom_edge_fan0 to visualize directed edges
fig.small_nb = ggraph(G.small_nb,layout = 'nicely') + 
  geom_edge_fan(aes(alpha = weight,colour= after_stat(index))) + 
  geom_node_point() +
  geom_node_text(aes(label = name),repel=T, segment.color = 'grey',segment.alpha = 0.5,alpha = 0.8) + 
  scale_edge_alpha(range = c(0.4,0.7)) +
  scale_edge_colour_gradient2(low = 'red',high='black',mid ='red',guide = "none") +
  theme_void() + 
  coord_fixed() + 
  ggtitle(lonelyWord)

print(fig.small_nb)
```



## PageRank and EigenCentrality
We have already seen an example of such a "feedback" measure before, the ``Katz Index``.  Another example, is ``PageRank``, which was a milestone in ranking important pages in the Google Search engine by Larry Page and Sergey Brin in 1996. Below we'll focus on ``PageRank`` as there is a fast implementation included in the ``igraph`` package. A second commonly used global centrality is ``Eigencentrality``. Like ``PageRank``,  it captures the intuition that a nodeâ€™s centrality in a network is determined by how central its neighbors. This amounts to summing the strengths of both direct and indirect neighbors. For more details, [Can Aksakalli](https://aksakalli.github.io/2017/07/17/network-centrality-measures-and-their-visualization.html) provides an visualization of common global centrality measures and a discussion of these measures deal with weakly connected components and directed vs undirected graphs.

```{r PRCentralityTable}
ct_pr = tibble(names = vlabels,
        pr_1 = igraph::page_rank(G,directed = TRUE,damping = 0.1)$vector,
        pr_20 = igraph::page_rank(G,directed = TRUE,damping = 0.2)$vector,
        pr_50 = igraph::page_rank(G,directed = TRUE,damping = 0.5)$vector,
        pr_70 = igraph::page_rank(G,directed = TRUE,damping = 0.7)$vector,
        eigen = igraph::eigen_centrality(G,directed = TRUE,scale = TRUE)$vector)
```

```{r echo=FALSE, warning=FALSE}
DT::datatable(cbind(ct_pr$names,round(ct_pr %>% select(-names),4)))

```

Determine what nodes are more central when taking into account global vs local centrality.
For small damping factors ($d = .01$) this is very similar to in-strength, $r = `r round(cor(ct_pr$pr_1,ct_k$strength_in,method = 'spearman'),5)`$. For larger values ($d = .5$), the correlation is somewhat smaller: $r = `r round(cor(ct_pr$pr_70,ct_k$strength_in,method = 'spearman'),5)`$.

Let's explore where the differences are situated by plotting extreme cases.
```{r }

X.ct = left_join(ct_k,ct_pr,by = 'names')
m_ct1 = lm(strength_in ~ pr_1,data = X.ct)
m_ct70 = lm(strength_in ~ pr_70,data = X.ct)
X.ct$cooks1 = cooks.distance(m_ct1)
X.ct$cooks70 = cooks.distance(m_ct70)
outlier1 = X.ct %>% slice_max(n = 30,order_by = abs(cooks1))
outlier70 = X.ct %>% slice_max(n = 30,order_by = abs(cooks70))

fig.pr1 = ggplot(X.ct,aes(x = strength_in,y = pr_1)) + 
  geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') + 
  geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
  geom_text_repel(data = X.ct %>% filter(names %in% outlier1$names),
                  aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 200) + 
  theme(aspect.ratio=1) + ggtitle('PageRank centrality alpha = 0.01')  + theme_bw()

fig.pr70 = ggplot(X.ct,aes(x = strength_in,y = pr_70)) + 
  geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') + 
  geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
  geom_text_repel(data = X.ct %>% filter(names %in% outlier70$names),
                  aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 200) + 
  theme(aspect.ratio=1) + ggtitle('PageRank centrality alpha = 0.70') + theme_bw()



# The following code might give you a memory error if used with large graphs...
#m_ct = lm(strength_in ~ eigen,data = X.ct)
#X.ct$cooks = cooks.distance(m_ct)
#outlier = X.ct %>% slice_max(n = 30,order_by = abs(cooks))


# fig.eig = ggplot(X.ct,aes(x = strength_in,y = eigen)) + 
#   geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') + 
#   geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
#   geom_text_repel(data = X.ct %>% filter(names %in% outlier$names),
#                   aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 200) + 
#   theme(aspect.ratio=1) +
#   ggtitle('Eigencentrality')


```

<center>
```{r echo=FALSE,fig.show=TRUE}
#grid.arrange(fig.eig, fig.pr, ncol = 2)
grid.arrange(fig.pr1, fig.pr70, ncol = 2)
```
</center>



The outliers with higher PageRank emphasizes adjectives (good,bad,cold) at the expense of concrete nouns (money, food, car, book). The results for Eigencentrality are harder to interpret.

## Betweenness
Another class of centrality measures is based on the number of shortest paths a nodes lies. A widely used path-based measures are *betweenness*.  According to the igraph documentation for betweenness: *The vertex and edge betweenness are (roughly) defined by the number of geodesics (shortest paths) going through a vertex or an edge.*

```{r betweennessCentralitY}
# Both betweenness for directed and undirected paths are considered
ct_b = tibble(names = vlabels, 
          betweenness = igraph::betweenness(G,directed = TRUE,normalized = T),
          betweennessU = igraph::betweenness(G,directed = FALSE,normalized = T)
        )

```


```{r echo=FALSE, warning=FALSE}
DT::datatable(cbind(ct_b$names,round(ct_b %>% select(-names),5)))

```

Compare the top 10 for directed and undirected versions of betweenness. What are the implications?


Is there a qualitative pattern to words where betweenness and in-strength are less related?
```{r}
X.ct = left_join(ct_nb,ct_k,by = 'names') %>% left_join(.,ct_pr,by = 'names') %>% left_join(.,ct_b,by ='names')
m_ct = lm(strength_in ~ betweenness,data = X.ct)
X.ct$cooks = cooks.distance(m_ct)

outlier = X.ct %>% slice_max(n = 40,order_by = abs(cooks))

fig.betw1 = ggplot(X.ct,aes(x = strength_in,y = betweenness)) + 
  geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') + 
  geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
  geom_text_repel(data = X.ct %>% filter(names %in% outlier$names),
                  aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 100,size =3) + 
  theme(aspect.ratio=1) + 
  ggtitle('Betweenness') + theme_bw()

# Let's zoom in on the low range
outlier = X.ct %>% filter(strength_in < 5) %>%  slice_max(n = 40,order_by = abs(cooks))

fig.betw2 = ggplot(X.ct %>% filter(strength_in<5),aes(x = strength_in,y = betweenness)) + 
  geom_point(size = 0.3,alpha = 0.3,colour = '#DD4814') + 
  geom_smooth(method = "lm", formula= y ~ x, se = FALSE,color = 'grey',size = 0.5) +
  geom_text_repel(data = X.ct %>% filter(names %in% outlier$names),
                  aes(label = names),segment.color = '#DD4814',segment.alpha=0.5,max.overlaps = 100,size = 3) + 
  theme(aspect.ratio=1) + ggtitle('Betweenness (Zoom)') + theme_bw()


```

<center>
```{r echo=FALSE,show.fig = TRUE}
grid.arrange(fig.betw1, fig.betw2, ncol = 2)
```
</center>


# Clustering measures
A somewhat different measure capture the degree of clustering of a node. Most natural networks are strongly clustered. The local clustering coefficient captures the transitivity between triples of nodes. For example, in a social network this transitivity would indicate that your friends might also be friends among themselves. More precisely, the local clustering coefficient $C_{i}$ for a vertex $v_{i}$ represents a proportion of the number of links between the vertices within its neighborhood divided by the number of links that could possibly exist between them.

Things quickly get complicated in weighted and directed graphs, and in practice clustering coefficients are not very robust.

```{r}

z = igraph::transitivity(G,type = 'localundirected',isolates = 'zero')
ct_cc = tibble(names = vlabels, 
               transitivity = z,
               cc = igraph::transitivity(simplify(G),type = 'barrat',isolates = 'zero'))
X.ct = left_join(X.ct,ct_cc,by = 'names')
```


Let's illustrate with two extreme example with a low (*pumpkin*) and high (*orchid*) clustering coefficient.

```{r}
v.cc.low = 'pumpkin'; v.cc.high = 'orchid'

G.cc.low = extractEgoGraph(G %>% as_tbl_graph(),v = v.cc.low,0.01)
G.cc.high = extractEgoGraph(G %>% as_tbl_graph(),v = v.cc.high,0.01)

# change geom_edge_link to geom_edge_fan0 to visualize directed edges
fig.cc.low = ggraph(G.cc.low,layout = 'igraph', algorithm = 'graphopt') + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point() +
  geom_node_text(aes(label = name),repel=T) + 
  scale_edge_alpha(range = c(0.15,0.3)) +
  theme_void() + 
  coord_fixed() + 
  ggtitle(v.cc.low)

fig.cc.high = ggraph(G.cc.high,layout = 'igraph', algorithm = 'fr') + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point() +
  geom_node_text(aes(label = name),repel=T) + 
  scale_edge_alpha(range = c(0.15,0.3)) +
  theme_void() + 
  coord_fixed() + 
  ggtitle(v.cc.high)

```

<center>
```{r echo=F,fig.show=T}

grid.arrange(fig.cc.low, fig.cc.high, ncol = 2)
```
</center>

# Evaluation
## Concreteness
Concreteness effects refer to an advantage in processing concrete over abstract words and this advantage has been attributed a wide range of factors. One explanation is that concrete words have higher contextual availability than abstract words [@Schwanenflugel1983]. According to this view, abstract words have a stronger dependence on context. Other studies have that the advantage of concrete words could be due to the number-of-features. Either way, it seems that distinct distributional factors could contribute and below we test whether different types of centrality are associated with concreteness.

We'll make use of the @Brysbaert2014 concreteness ratings for a large set of English words.

```{r correlationConcreteness}
file.concreteness = '../data/concreteness/english.concreteness.Brysbaert2016.csv'
X.con = readr::read_delim(file.concreteness,delim = '\t')
X.con = inner_join(X.ct,X.con,by = c('names'= 'Word'))

rs  = corrTable(X.con %>% select(-names,-Dom_Pos),rmethod='spearman',absolute = TRUE)

fig.conc = ggplot(rs %>% filter(var1 =='Conc.M',
                         var2 %in% c('nb2_out','nb3_out','degree_out','transitivity','betweenness','betweennessU',
                                                     'pr_1','pr_70','strength_in','cc','SUBTLEX')), 
           aes(x=reorder(var2, var2, function(x) length(x)), y=r)) + 
        geom_bar(stat="identity", aes(fill = sign), alpha = 0.4) +
        scale_fill_manual(values=c("blue", "red")) + 
        geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + theme_bw() + xlab('Centrality Measure') +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

<center>
```{r echo=F,fig.show=T}
fig.conc
```
</center>

So there seems to be a weak correlation between concreteness and out-degree that is strong when taking into account the indirect neighbors as well, although the difference is small.

## Semantic Richness effects
Words that are semantically rich have  processing advantage (e.g. shorter RTs across a range of lexical tasks).
 There are many ways in which semantic richness can be derived. This could be a matter of number of shared features, number of distinct features, contextual dispersion, number of semantic neighbors, number of senses, and so on [e.g. @Pexman2017]. Some of these measures reflect the way features are counted, and potentially a further distinction can be drawn based on *local* and *global* measures of centrality, such as degree measures based on direct or indirect neighbors.
 
A first dataset that can shed some light on this is the CSDP data. These data include semantic decisions where participants are judge whether a word is concrete or abstract. It contains both RTs and accuracy for over 40,000 words collected from more than 800 participants.

```{r correlationCSDP}
# Load Calgary Semantic Decision Project Data
file.CSD = '../data/CalgarySemanticDecision/CalgarySemanticDecisionSummary.csv'
X.csdp = read_csv(file.CSD) 
X.csdp %<>% select(word,wordType,RT = zRTclean_mean,accuracy = ACC,concreteRating)

X.csdp = inner_join(X.csdp,X.ct,by = c('word' = 'names')) %>% filter(complete.cases(.))
DT::datatable(as.data.frame(round(cor(X.csdp %>% select(-word,-wordType),method = 'spearman')[,c('RT','accuracy','concreteRating')],3)))
```

```{r echo = FALSE}
rs.csdp  = corrTable(X.csdp %>% select(-word,-wordType),rmethod='spearman',absolute = TRUE)

fig.csdp = ggplot(rs.csdp %>% filter(var1 =='RT',
                         var2 %in% c('nb2_out','nb3_out','degree_out','cc','betweenness','betweennessU',
'transitivity','pr_1','pr_70','strength_in','SUBTLEX')), 
           aes(x=reorder(var2, var2, function(x) length(x)), y=r)) + 
        geom_bar(stat="identity", aes(fill = sign), alpha = 0.4) +
        scale_fill_manual(values=c("blue", "red")) + 
        geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + 
        theme_minimal() + xlab('Centrality Measure') +
        theme(axis.text.x = element_text(angle = 45,hjust = 1))
```

<center>
```{r echo = FALSE,show.fig = TRUE}
fig.csdp
```
</center>

Note how the strongest correlations (in an absolute sense) are based on global measures like indirect neighborhood out-degree, eigencentrality or PageRank. Notably, we see that out-degree measures have a positive correlation (slower RTs) whereas in-degree measures have a negative correlation. Shortest path-based measured such as betweenness do not perform that well.


Let's see how this compares to other types of predictors taken from the English Crowdsourcing Project (ECP) word prevalence data
```{r}
# Correct the skew in nb2_in
m.csdp = lm(RT ~ log(nb2_in+1) + (nb2_out), data = X.csdp)
print(summary(m.csdp))

```

Finally, let's take at the @Mandera2020 English Word Prevalence RT data. These RTs measure how long it takes for participants to judge whether they "know" a word.
```{r}
file.prevalence = '../data/EnglishLexiconProject/ELPzScores.csv'
X.ecd = read_csv(file.prevalence)
X.ecd  %<>% select(word = Word,
                    length = Length, 
                    ON = Ortho_N,prevalence = Prevalence,
                    ECP_RT,
                    zRT = zECP_24,
                    WF = SubtlexZipf)

X.csdp2 = inner_join(X.csdp,X.ecd,by ='word')

rs.ecd  = corrTable(X.csdp2 %>% select(-word,-wordType),
                    rmethod='spearman',absolute = TRUE)

fig.ecd = ggplot(rs.ecd %>% filter(var1 =='zRT',
             var2 %in% c('degree_out','cc','betweenness','betweennessU',
                         'eigen','pr_70','strength_in','WF')), 
           aes(x=reorder(var2, var2, function(x) length(x)), y=r)) + 
        geom_bar(stat="identity", aes(fill = sign), alpha = 0.5) +
        scale_fill_manual(values=c("blue", "red")) + 
        geom_errorbar(aes(ymin=cl, ymax=cu), width=.2)  + 
        theme_minimal() + 
        xlab('Centrality Measure') +
        theme(axis.text.x = element_text(angle = 45,hjust = 1))
```


<center>
```{r echo = FALSE,show.fig = TRUE}
fig.ecd
```
</center>


# Exercises
* There are many centrality values and its far from obvious which one is best to use. One criterion could be the robustness of the measure. Calculate the centrality for the `EAT` and `Sw.en` networks as well and correlate the centrality across these three networks. How certain are you that the values are comparable? What factors play a role? Why are certain measures more robust than others?


* So far we have used associative strength to construct weighted networks. What do you expect for `PPMI` or `RW` networks? Hint: Think about the type of semantic relations and the density of the graph.


# Further reading
A great resource for network analysis in R is the [Introduction to R for networks](https://kateto.net/wp-content/uploads/2016/01/NetSciX_2016_Workshop.pdf) page by Katherine Ongnayanova. It will provide you with a refresher of basic R knowledge and builds up your skills to work with `igraph`.

Several studies have tried to compare local and global graph properties across different groups (e.g. depending on creativity, see, or lanugage see yy). There isn't enought time to go over the methodological and statistical challenges associated with this method, but recent work by @robinson2023 provides some useful guidelines on how to perform such measurements and which measures are robust or not. Local but not global graph theoretic measures of semantic networks generalize across tasks


# References

